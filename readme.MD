# WebGen Service — Project Vision

## What It Does

WebGen is an async HTTP service that takes a Markdown snapshot of a scraped website
and returns a freshly designed, single-file HTML landing page — generated by a local
open-source LLM hosted on Vast.ai GPU infrastructure.

**Input:** `snapshot.md` — scraped site content (headings, text, links, tables)  
**Output:** `{domain}.zip` — complete `index.html` ready to serve

---

## Business Context

Target sites are **financial advisory / fintech landing pages** (retirement plan advisors,
wealth management firms). Every generated site must contain three Rixtrema-specific
interactive sections that call the live Rixtrema API:

1. **News & AI Briefing** — latest market news + AI-synthesized executive brief
2. **Plan Search** — incremental search of 401(k) plans by company name or EIN
3. **Plan Strategy** — AI-generated strengths/deficiencies/action plan for a selected plan

These sections are template-injected (not LLM-generated) to guarantee correctness of API calls.
The LLM adapts their visual styling to match the overall design of each site.

---

## Architecture

```
Client script
    │
    ├─ POST /start  {"snapshot": "..."}
    │       └─► returns {"website_id": uuid}
    │
    ├─ GET  /status/{id}     ← poll until status = "done" or "error"
    ├─ GET  /download/{id}   ← returns .zip
    └─ GET  /admin           ← admin SPA (view/delete jobs)

[FastAPI service, port 7860]
    │
    └─ asyncio.Queue → single JobWorker
            │
            ├─ Preprocessor
            │     ├─ extract domain from snapshot header
            │     ├─ detect blocked/empty snapshots → error immediately
            │     ├─ split into page sections
            │     ├─ filter FAILED pages
            │     ├─ deduplicate identical pages (e.g. home listed twice)
            │     ├─ strip repeated nav/menu blocks (keep only in first page)
            │     ├─ prioritize pages: root > services > contact > blog > legal
            │     └─ truncate to 48k tokens if over limit
            │
            ├─ LLM Client → vLLM (localhost:8000, OpenAI-compatible)
            │     ├─ build design seed from MD5(domain) → style + layout + palette
            │     ├─ build user prompt with filtered content + section templates
            │     ├─ call Qwen2.5-Coder-32B-AWQ, max_tokens=12288, temp=0.7
            │     └─ continuation pass if output was truncated (finish_reason=length)
            │
            └─ Postprocessor
                  ├─ strip markdown code fences if model added them
                  ├─ validate HTML structure (html.parser)
                  ├─ check all 3 required sections are present
                  └─ pack index.html + README.txt → {job_id}.zip

[vLLM server, port 8000]
    └─ Qwen/Qwen2.5-Coder-32B-Instruct-AWQ
         AWQ 4-bit quantization
         tensor-parallel-size 2 (for 2× A100 40GB) or 1 (for 1× A100 80GB)
         max-model-len 65536

[SQLite database]
    └─ jobs table: id, domain, status, created_at, snapshot_tokens, strategy, error, zip_path
```

---

## Design System

Each site gets a **deterministic, unique** design derived from its domain using MD5 hashing.
This means:
- The same domain always gets the same design
- Different domains get different designs
- No two consecutive sites look alike

**Design dimensions** (selected independently via different bit ranges of the hash):

| Dimension | Options |
|-----------|---------|
| Visual style | minimalist brutalist, warm editorial, dark tech, luxury minimal, corporate clean, glassmorphism, earthy organic, neo-brutalist, fintech precision, startup bold, aurora gradient, monochrome professional, financial advisory premium, data-driven dashboard, bold typographic |
| Layout | asymmetric grid, full-width sections, magazine layout, single column editorial, bento grid, hero-first split, card-dominant |
| Typography | large expressive headings + tight monospace body, serif editorial headlines + clean sans, geometric sans throughout, mixed serif-sans, system font stack + bold weight contrast, condensed display headings |
| Accent palette | 12 finance-appropriate palettes (deep navy, teal professional, purple premium, emerald growth, amber warm, etc.) |

The LLM receives the design seed and is instructed to:
- Use the accent hue as HSL CSS custom property
- Configure Tailwind with a custom color theme
- Make the hero section visually distinctive
- NOT use generic #3B82F6 blue unless it fits the palette
- Vary section layouts — not all centered icon+text grids

---

## Snapshot Processing

### What we see in production data

| Snapshot | Raw size | After filter | Strategy |
|----------|----------|--------------|----------|
| Small (1–4 pages) | 1–15k tokens | 0.5–5k | single_pass |
| Medium (5–15 pages) | 15–40k tokens | 8–20k | single_pass |
| Large (15–25 pages) | 40–90k tokens | 20–45k | single_pass |
| Massive (25+ pages) | 90k+ tokens | 45–60k | truncated (priority) |
| Blocked (Cloudflare) | < 1k tokens | — | error |

### Pre-filtering steps (in order)

1. **Detect blocked/empty** — < 5 meaningful lines or "BLOCKED (Cloudflare)" → immediate error
2. **Split into sections** — on `## Page Title` boundaries
3. **Remove FAILED pages** — pages that errored during scraping
4. **Deduplicate pages** — identical pages (home listed twice) removed by content fingerprint
5. **Strip repeated nav blocks** — navigation link lists kept only in first section
6. **Priority truncation** (if still > 48k tokens):
   - Keep: root page, services/about, contact
   - Deprioritize: blog posts, legal pages
   - Drop: anything over budget (highest priority pages preserved whole)

---

## API Reference

### POST /start
```json
{ "snapshot": "# www.example.com\n..." }
```
Returns:
```json
{ "website_id": "550e8400-e29b-41d4-a716-446655440000" }
```

### GET /status/{id}
```json
{
  "status": "pending | processing | done | error",
  "domain": "www.example.com",
  "snapshot_tokens": 12500,
  "strategy": "single_pass | truncated",
  "error": null,
  "created_at": "2026-02-20T14:22:00",
  "updated_at": "2026-02-20T14:24:37"
}
```

### GET /download/{id}
Returns `application/zip` with `index.html` + `README.txt`.

### GET /admin
Admin SPA — dark-theme table of all jobs with download/delete actions.

### GET /jobs
Raw JSON list of all jobs (used by admin panel).

### DELETE /jobs/{id}
Deletes job record and its ZIP file from disk.

---

## Infrastructure — Vast.ai

### Recommended instance

| Option | GPU | VRAM | Disk | $/hr est. | Notes |
|--------|-----|------|------|-----------|-------|
| **Recommended** | 1× A100 80GB SXM | 80GB | 100GB | ~$2.5–3.5 | Simpler setup, 128k context possible |
| Budget | 2× A100 40GB PCIe | 80GB total | 100GB | ~$3–5 | tensor_parallel=2, PCIe bandwidth |
| Overkill | 2× A100 80GB SXM | 160GB | 100GB | ~$5–8 | For batch throughput |

**Do NOT use:**
- RTX 4090 / 3090 — VRAM too small for 32B even with AWQ
- T4 / V100 — too slow, limited VRAM

### Disk sizing

| Component | Size |
|-----------|------|
| OS + CUDA drivers | ~15 GB |
| Python + vLLM + torch | ~15 GB |
| Qwen2.5-32B-AWQ model | ~18 GB |
| Generated ZIPs (buffer) | ~2 GB (10k sites × 50KB) |
| Logs + SQLite | < 1 GB |
| **Total minimum** | **~55 GB** |
| **Recommended disk** | **80 GB** |

### Startup sequence

1. Vast.ai runs `startup_bootstrap.sh` (< 2000 chars, within template limit)
2. Bootstrap downloads the full `startup.sh` from GitHub
3. `startup.sh`:
   - Installs Python deps (vLLM, FastAPI, httpx, uvicorn)
   - Clones/pulls the repo from GitHub
   - Downloads the model from HuggingFace (using `HF_TOKEN` env var)
   - Starts vLLM on port 8000, waits for health check
   - Starts FastAPI on port 7860

**Required Vast.ai env vars:**
- `HF_TOKEN` — HuggingFace token (for faster model download)

**Optional overrides:**
- `WEBGEN_MODEL` — model ID (default: `Qwen/Qwen2.5-Coder-32B-Instruct-AWQ`)
- `WEBGEN_GPU_COUNT` — tensor parallel size (default: `2`)
- `WEBGEN_MAX_MODEL_LEN` — context window (default: `65536`)
- `WEBGEN_PORT` — FastAPI port (default: `7860`)

---

## Performance Expectations

| GPU | Model | Tokens/sec | Time per site (8k output) | Sites/day |
|-----|-------|-----------|--------------------------|-----------|
| 1× A100 80GB | Qwen-32B-AWQ | ~80–100 t/s | ~80–100 sec | ~850–1000 |
| 2× A100 40GB PCIe | Qwen-32B-AWQ | ~70–90 t/s | ~90–110 sec | ~780–960 |
| 1× A100 40GB | Qwen-32B-AWQ | ~60–80 t/s | ~100–130 sec | ~660–860 |

*Single worker, sequential processing. vLLM prefill (~5–10 sec) not counted.*

---

## Repository Structure

```
github.com/greenbutton75/webgen-service

├── startup_bootstrap.sh        # Vast.ai startup script (< 2000 chars)
├── startup.sh                  # Full startup: vLLM + service
├── requirements.txt            # Python deps for FastAPI service
├── .env.example                # Env var template
├── .gitignore
├── commit.bat                  # Windows git helper
├── VISION.md                   # This file
│
├── service/
│   ├── main.py                 # FastAPI app
│   ├── worker.py               # Async job worker (single)
│   ├── db.py                   # SQLite helpers
│   ├── preprocessor.py         # MD pre-filtering + token estimation
│   ├── llm_client.py           # vLLM client + continuation logic
│   ├── postprocessor.py        # HTML validation + ZIP packaging
│   └── design_seed.py          # Deterministic design from domain hash
│
├── prompts/
│   ├── system.txt              # LLM system prompt
│   └── sections/
│       ├── news.html           # News + AI Briefing section template
│       └── plan_search_strategy.html  # Plan search + strategy template
│
├── admin/
│   └── index.html              # Dark-theme admin SPA (Alpine.js)
│
└── tests/
    └── test_local.py           # Local testing (preprocessor + design seed)
```

---

## Key Design Decisions

### Why template-inject the 3 required sections instead of LLM-generating them?
- **Correctness**: API endpoints, request format, and response parsing must be exact.
  LLMs hallucinate URLs and JSON structures.
- **Efficiency**: Saves ~2–4k output tokens per site.
- **Consistency**: All sites will have the same working JS logic regardless of model behavior.
- **LLM still adapts styling**: The system prompt tells the model to integrate the templates
  into its design — adapting colors, borders, shadows.

### Why SQLite instead of Redis/Celery?
- Zero external dependencies on the Vast.ai instance
- Survives if the service restarts (unlike asyncio.Queue)
- Sufficient for single-worker throughput
- Easy to inspect with any SQLite tool

### Why single worker instead of multiple?
- vLLM already handles one generation at a time efficiently
- Multiple workers would compete for the same vLLM process, adding overhead
- Simpler error handling and logging

### Why continuation pass for truncated output?
- LLMs frequently hit `max_tokens` before closing `</html>` on complex sites
- Continuation with `temperature=0` produces deterministic, minimal closing HTML
- Better than hard-failing the job

### Why AWQ quantization?
- Reduces model VRAM from ~64GB (BF16) to ~18GB (AWQ 4-bit)
- Makes 1× A100 40GB viable (fits with room for KV cache)
- Perplexity degradation is minimal for code/HTML generation tasks
- vLLM has first-class AWQ support with kernel-level optimization

---

## Future Improvements (Not Yet Implemented)

- **Retry with lower temperature** if HTML validation fails (missing required sections)
- **Hierarchical generation** for very large sites (skeleton pass + content fill pass)
- **Webhook support** — POST to a URL when job completes instead of polling
- **Batch client** — submit multiple snapshots, get progress on all
- **Preview endpoint** — serve the generated HTML directly at `/preview/{id}`
- **Quality scoring** — automatic scoring of generated sites (valid HTML, sections present, design variety)
